# ETL pipeline to process datalake with Spark 
This etl pipeline extracts data from datalake hosted on S3, process the data using Spark and load data to a new s3 bucket as a set of dimensional tables in parquet format. 

## Repository structure ðŸ“‚
`dl.cg` contains aws credentials <br>
`etl.py` is a python script that extracts, tranforms and loads data to S3.

## Data Modeling ðŸ’¾
Two distinct data sources were used and stored in different s3 buckets:   
- Sub dataset of ["Milion Song Dataset"](http://millionsongdataset.com/) contains songs info and metadata.
- Logs and info generated by ['Eventsim'](https://github.com/Interana/eventsim) to simulate real users usage.

A denormalized star schema was created to model the data: 
![img](https://raw.githubusercontent.com/natalianrs/ETL_Pipeline_PostgreSQL/main/data_model_postgre.png)
 

## ETL Data Pipeline Script âš™
The etl.py file runs the pipeline by:
- extracting data from different s3 buckets 
- processing data using spark 
- loading denormalized data into a new s3 bucket

## How to run etl pipeline ðŸ’¡
	1. Login to AWS account 
	2. Launch EMR Cluster 
	3. Create IAM user and save access credentials
	4. Run `python etl.py` 
	
âœ¨ 
